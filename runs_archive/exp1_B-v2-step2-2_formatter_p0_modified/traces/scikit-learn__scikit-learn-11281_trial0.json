{
  "task_id": "scikit-learn__scikit-learn-11281",
  "trial_id": 0,
  "context_used": true,
  "context_num_files": 80,
  "repo_context_preview": "Existing files (choose from these):\nsetup.py\nconftest.py\ndoc/conf.py\ndoc/conftest.py\nsklearn/base.py\nsklearn/dummy.py\nsklearn/setup.py\nsklearn/impute.py\nsklearn/_config.py\nsklearn/__init__.py\nsklearn/isotonic.py\nsklearn/pipeline.py\nsklearn/svm/base.py\nsklearn/svm/setup.py\nsklearn/tree/tree.py\nsklearn/exceptions.py\nsklearn/multiclass.py\nsklearn/svm/bounds.py\nsklearn/tree/setup.py\nsklearn/calibration.py\n... (+60 more)",
  "format_used": true,
  "format_ok": true,
  "format_reason": "git_apply_corrupt_patch:error: corrupt patch at line 36",
  "issue_text": null,
  "test_command": "git apply",
  "diff": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 3a4b5c6..d7e8f9g 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -10,6 +10,7 @@\n from sklearn.base import BaseEstimator, ClusterMixin, BiclusterMixin, TransformerMixin\n from sklearn.utils.validation import check_array, check_is_fitted\n from sklearn.utils.extmath import logsumexp\n+from sklearn.cluster._kmeans import _check_sample_weight\n \n \n class BaseMixture(BaseEstimator, ClusterMixin):\n@@ -20,6 +21,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n     def __init__(self, n_components=1, tol=1e-3, reg_covar=1e-6,\n                  max_iter=100, n_init=1, init_params='kmeans',\n                  random_state=None, warm_start=False, verbose=0,\n+                 verbose_interval=10, copy_x=True, n_jobs=None,\n                  algorithm='em'):\n         self.n_components = n_components\n         self.tol = tol\n@@ -30,6 +32,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.max_iter = max_iter\n         self.n_init = n_init\n         self.init_params = init_params\n+        self.random_state = random_state\n         self.warm_start = warm_start\n         self.verbose = verbose\n         self.verbose_interval = verbose_interval\n@@ -40,6 +43,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.copy_x = copy_x\n         self.n_jobs = n_jobs\n         self.algorithm = algorithm\n \n     def fit(self, X, y=None, sample_weight=None):\n         \"\"\"Fit the model to the data using EM algorithm.\n@@ -50,6 +54,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 Training data.\n             y : Ignored\n+            sample_weight : array-like, shape (n_samples,), optional\n                 Weights assigned to individual samples.\n \n         Returns\n@@ -60,6 +65,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         \"\"\"\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=2)\n         self._validate_params()\n+        sample_weight = _check_sample_weight(sample_weight, X)\n \n         best_parameters, log_prob = None, None\n         for init in range(self.n_init):\n@@ -70,6 +76,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 parameters = self._initialize(X, random_state=random_state)\n                 log_prob_i, params_i = self._em(\n                     X, parameters, random_state=random_state)\n+                log_prob_i += np.sum(sample_weight * log_prob_i)\n                 if best_log_prob is None or log_prob_i > best_log_prob:\n                     best_parameters = params_i\n                     best_log_prob = log_prob_i\n@@ -80,6 +87,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self._set_parameters(best_parameters)\n         self.converged_ = True\n         return self\n+\n     def predict(self, X):\n         \"\"\"Predict the labels for the data samples in X using trained model.\n \n@@ -90,6 +98,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 Data to predict on.\n \n+        Returns\n             labels : array, shape (n_samples,)\n                 Component labels.\n         \"\"\"\n@@ -100,6 +109,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob = self._estimate_log_prob(X)\n         return log_prob.argmax(axis=1)\n+\n     def predict_proba(self, X):\n         \"\"\"Predict posterior probability of membership for each component for\n         each sample in X.\n \n@@ -110,6 +120,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 Data to predict on.\n \n+        Returns\n             responsibilities : array, shape (n_samples, n_components)\n                 Posterior probabilities of each component for each sample.\n         \"\"\"\n@@ -120,6 +131,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob = self._estimate_log_prob(X)\n         return np.exp(log_prob)\n+\n     def score_samples(self, X):\n         \"\"\"Compute the per-sample average log-likelihood of the given data\n         samples under the current model.\n \n@@ -130,6 +142,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 List of n_features-dimensional data points. Each row corresponds\n                 to a single data point.\n+        Returns\n             log_likelihood : array, shape (n_samples,)\n                 Log likelihood of each sample under the current model.\n         \"\"\"\n@@ -140,6 +153,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob = self._estimate_log_prob(X)\n         return log_prob.mean(axis=1)\n+\n     def score(self, X, y=None):\n         \"\"\"Compute the per-sample average log-likelihood of the given data\n         samples under the current model.\n \n@@ -150,6 +164,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 List of n_features-dimensional data points. Each row corresponds\n                 to a single data point.\n+        Returns\n             log_likelihood : float\n                 Average log likelihood of all samples under the current model.\n         \"\"\"",
  "stdout": "",
  "stderr": "Git Apply Failed: error: corrupt patch at line 36\n",
  "repo_commit": null,
  "docker_image": "swebench/sweb.eval.x86_64:latest",
  "model_config": null,
  "full_result": {
    "task_id": "scikit-learn__scikit-learn-11281",
    "trial_id": 0,
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "prompt_hash": "2e87a44e17ad9283d5acc4eb2fcd2370e6363b188d38c8c7ac691db8586efa35",
    "diff": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 3a4b5c6..d7e8f9g 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -10,6 +10,7 @@\n from sklearn.base import BaseEstimator, ClusterMixin, BiclusterMixin, TransformerMixin\n from sklearn.utils.validation import check_array, check_is_fitted\n from sklearn.utils.extmath import logsumexp\n+from sklearn.cluster._kmeans import _check_sample_weight\n \n \n class BaseMixture(BaseEstimator, ClusterMixin):\n@@ -20,6 +21,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n     def __init__(self, n_components=1, tol=1e-3, reg_covar=1e-6,\n                  max_iter=100, n_init=1, init_params='kmeans',\n                  random_state=None, warm_start=False, verbose=0,\n+                 verbose_interval=10, copy_x=True, n_jobs=None,\n                  algorithm='em'):\n         self.n_components = n_components\n         self.tol = tol\n@@ -30,6 +32,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.max_iter = max_iter\n         self.n_init = n_init\n         self.init_params = init_params\n+        self.random_state = random_state\n         self.warm_start = warm_start\n         self.verbose = verbose\n         self.verbose_interval = verbose_interval\n@@ -40,6 +43,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.copy_x = copy_x\n         self.n_jobs = n_jobs\n         self.algorithm = algorithm\n \n     def fit(self, X, y=None, sample_weight=None):\n         \"\"\"Fit the model to the data using EM algorithm.\n@@ -50,6 +54,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 Training data.\n             y : Ignored\n+            sample_weight : array-like, shape (n_samples,), optional\n                 Weights assigned to individual samples.\n \n         Returns\n@@ -60,6 +65,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         \"\"\"\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=2)\n         self._validate_params()\n+        sample_weight = _check_sample_weight(sample_weight, X)\n \n         best_parameters, log_prob = None, None\n         for init in range(self.n_init):\n@@ -70,6 +76,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 parameters = self._initialize(X, random_state=random_state)\n                 log_prob_i, params_i = self._em(\n                     X, parameters, random_state=random_state)\n+                log_prob_i += np.sum(sample_weight * log_prob_i)\n                 if best_log_prob is None or log_prob_i > best_log_prob:\n                     best_parameters = params_i\n                     best_log_prob = log_prob_i\n@@ -80,6 +87,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self._set_parameters(best_parameters)\n         self.converged_ = True\n         return self\n+\n     def predict(self, X):\n         \"\"\"Predict the labels for the data samples in X using trained model.\n \n@@ -90,6 +98,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 Data to predict on.\n \n+        Returns\n             labels : array, shape (n_samples,)\n                 Component labels.\n         \"\"\"\n@@ -100,6 +109,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob = self._estimate_log_prob(X)\n         return log_prob.argmax(axis=1)\n+\n     def predict_proba(self, X):\n         \"\"\"Predict posterior probability of membership for each component for\n         each sample in X.\n \n@@ -110,6 +120,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 Data to predict on.\n \n+        Returns\n             responsibilities : array, shape (n_samples, n_components)\n                 Posterior probabilities of each component for each sample.\n         \"\"\"\n@@ -120,6 +131,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob = self._estimate_log_prob(X)\n         return np.exp(log_prob)\n+\n     def score_samples(self, X):\n         \"\"\"Compute the per-sample average log-likelihood of the given data\n         samples under the current model.\n \n@@ -130,6 +142,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 List of n_features-dimensional data points. Each row corresponds\n                 to a single data point.\n+        Returns\n             log_likelihood : array, shape (n_samples,)\n                 Log likelihood of each sample under the current model.\n         \"\"\"\n@@ -140,6 +153,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob = self._estimate_log_prob(X)\n         return log_prob.mean(axis=1)\n+\n     def score(self, X, y=None):\n         \"\"\"Compute the per-sample average log-likelihood of the given data\n         samples under the current model.\n \n@@ -150,6 +164,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             X : array-like, shape (n_samples, n_features)\n                 List of n_features-dimensional data points. Each row corresponds\n                 to a single data point.\n+        Returns\n             log_likelihood : float\n                 Average log likelihood of all samples under the current model.\n         \"\"\"",
    "patch_lines_added": 14,
    "patch_lines_removed": 0,
    "files_changed": 1,
    "timestamp": "2026-02-17T10:09:49.030025",
    "seed": 42,
    "repo": "scikit-learn/scikit-learn",
    "base_commit": "4143356c3c51831300789e4fdf795d83716dbab6",
    "taxonomy_version": "B-v2-step2-2",
    "gen_elapsed_sec": 13.548816680908203,
    "context_used": true,
    "context_num_files": 80,
    "repo_context_preview": "Existing files (choose from these):\nsetup.py\nconftest.py\ndoc/conf.py\ndoc/conftest.py\nsklearn/base.py\nsklearn/dummy.py\nsklearn/setup.py\nsklearn/impute.py\nsklearn/_config.py\nsklearn/__init__.py\nsklearn/isotonic.py\nsklearn/pipeline.py\nsklearn/svm/base.py\nsklearn/svm/setup.py\nsklearn/tree/tree.py\nsklearn/exceptions.py\nsklearn/multiclass.py\nsklearn/svm/bounds.py\nsklearn/tree/setup.py\nsklearn/calibration.py\n... (+60 more)",
    "format_used": true,
    "format_ok": true,
    "format_reason": "git_apply_corrupt_patch:error: corrupt patch at line 36",
    "stage": "PATCH",
    "stdout": "",
    "stderr": "Git Apply Failed: error: corrupt patch at line 36\n",
    "returncode": 128,
    "timeout": false,
    "elapsed_sec": 0.4936063289642334,
    "test_command": "git apply",
    "success": false,
    "error_type": "PATCH_FAIL",
    "signature": "git_apply_corrupt_patch",
    "docker_image": "swebench/sweb.eval.x86_64:latest"
  }
}