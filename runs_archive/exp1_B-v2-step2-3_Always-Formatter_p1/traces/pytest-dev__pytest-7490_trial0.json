{
  "task_id": "pytest-dev__pytest-7490",
  "trial_id": 0,
  "context_used": true,
  "context_num_files": 80,
  "repo_context_preview": "Existing files (choose from these):\nsetup.py\nbench/skip.py\nbench/bench.py\nbench/empty.py\nbench/xunit.py\ndoc/en/conf.py\nbench/manyparam.py\nbench/unit_test.py\ndoc/en/conftest.py\nscripts/release.py\nextra/get_issues.py\nsrc/_pytest/main.py\nsrc/_pytest/nose.py\ntesting/conftest.py\nsrc/_pytest/nodes.py\nsrc/_pytest/scope.py\nsrc/_pytest/stash.py\ntesting/test_main.py\ntesting/test_mark.py\ntesting/test_meta.py\n... (+60 more)",
  "format_used": false,
  "format_ok": false,
  "format_reason": "",
  "issue_text": null,
  "test_command": "",
  "diff": "",
  "stdout": "",
  "stderr": "LLM call failed (provider=vllm, base_url=http://localhost:8000/v1, model=Qwen/Qwen2.5-Coder-7B-Instruct): Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your request has 4978 input tokens. Please reduce the length of the input messages. None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
  "repo_commit": null,
  "docker_image": null,
  "model_config": null,
  "full_result": {
    "task_id": "pytest-dev__pytest-7490",
    "trial_id": 0,
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "prompt_hash": "b16ae4584dc8c38e049b610a387604c51bc8e57a364f5be08aa8b9f28a27bf86",
    "diff": "",
    "patch_lines_added": 0,
    "patch_lines_removed": 0,
    "files_changed": 0,
    "timestamp": "2026-02-17T13:34:54.357923",
    "seed": 42,
    "repo": "pytest-dev/pytest",
    "base_commit": "7f7a36478abe7dd1fa993b115d22606aa0e35e88",
    "taxonomy_version": "B-v2-step2-2",
    "gen_elapsed_sec": 0.015245914459228516,
    "context_used": true,
    "context_num_files": 80,
    "repo_context_preview": "Existing files (choose from these):\nsetup.py\nbench/skip.py\nbench/bench.py\nbench/empty.py\nbench/xunit.py\ndoc/en/conf.py\nbench/manyparam.py\nbench/unit_test.py\ndoc/en/conftest.py\nscripts/release.py\nextra/get_issues.py\nsrc/_pytest/main.py\nsrc/_pytest/nose.py\ntesting/conftest.py\nsrc/_pytest/nodes.py\nsrc/_pytest/scope.py\nsrc/_pytest/stash.py\ntesting/test_main.py\ntesting/test_mark.py\ntesting/test_meta.py\n... (+60 more)",
    "format_used": false,
    "format_ok": false,
    "format_reason": "",
    "stdout": "",
    "stderr": "LLM call failed (provider=vllm, base_url=http://localhost:8000/v1, model=Qwen/Qwen2.5-Coder-7B-Instruct): Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your request has 4978 input tokens. Please reduce the length of the input messages. None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
    "returncode": null,
    "timeout": false,
    "elapsed_sec": 0.015245914459228516,
    "signature": "llm_call_fail",
    "test_command": "",
    "stage": "GEN",
    "error_type": "GEN_FAIL",
    "success": false
  }
}