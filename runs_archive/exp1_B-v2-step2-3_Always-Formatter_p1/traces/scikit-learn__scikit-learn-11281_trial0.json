{
  "task_id": "scikit-learn__scikit-learn-11281",
  "trial_id": 0,
  "context_used": true,
  "context_num_files": 80,
  "repo_context_preview": "Existing files (choose from these):\nsetup.py\nconftest.py\ndoc/conf.py\ndoc/conftest.py\nsklearn/base.py\nsklearn/dummy.py\nsklearn/setup.py\nsklearn/_config.py\nsklearn/__init__.py\nsklearn/conftest.py\nsklearn/isotonic.py\nsklearn/pipeline.py\nsklearn/svm/base.py\nsklearn/svm/setup.py\nsklearn/tree/tree.py\nsklearn/exceptions.py\nsklearn/multiclass.py\nsklearn/svm/bounds.py\nsklearn/tree/setup.py\nsklearn/calibration.py\n... (+60 more)",
  "format_used": true,
  "format_ok": false,
  "format_reason": "formatter_exception:Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 4096 tokens and your request has 2488 input tokens (2048 > 4096 - 2488). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
  "issue_text": null,
  "test_command": "",
  "diff": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 1a2b3c4..5d6e7f8 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -10,6 +10,7 @@\n from sklearn.base import BaseEstimator, ClusterMixin, BiclusterMixin, TransformerMixin\n from sklearn.utils.validation import check_array, check_is_fitted\n from sklearn.utils.extmath import logsumexp\n+from sklearn.cluster._kmeans import _check_cluster_parameters\n \n \n class BaseMixture(BaseEstimator, ClusterMixin):\n@@ -20,6 +21,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n     def __init__(self, n_components=1, tol=1e-3, reg_covar=1e-6,\n                  max_iter=100, n_init=1, init_params='kmeans',\n                  random_state=None, warm_start=False, verbose=0,\n+                 verbose_interval=10, copy_x=True):\n         self.n_components = n_components\n         self.tol = tol\n         self.reg_covar = reg_covar\n@@ -30,6 +32,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.random_state = random_state\n         self.warm_start = warm_start\n         self.verbose = verbose\n+        self.verbose_interval = verbose_interval\n         self.copy_x = copy_x\n \n     def fit(self, X, y=None):\n@@ -40,6 +43,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         \"\"\"\n         Fit the model for X.\n \n+        Parameters\n         X : array-like, shape (n_samples, n_features)\n             Training data.\n \n@@ -50,6 +54,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         \"\"\"\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=2)\n         self._validate_params()\n+        self._check_n_components(X.shape[0])\n \n         # If init_params == 'kmeans', use k-means initialization unless it was\n         # overridden with init_params via a dict setting.\n@@ -60,6 +65,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             init_params = {'means': init_params}\n \n         best_parameters, best_log_prob = None, None\n+        n_trials = 0\n \n         for trial in range(self.n_init):\n             if self.verbose > 0:\n@@ -70,6 +76,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 print(\"Initialization %d\" % (trial + 1))\n \n             params = self._initialize(X, init_params, random_state=self.random_state)\n+            n_trials += 1\n \n             log_prob, responsibilities, means, covariances, weights = self._e_step(X, params)\n             log_prob_new, params_new = self._m_step(X, responsibilities, means, covariances, weights)\n@@ -80,6 +87,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 break\n \n             params = params_new\n+            n_trials += 1\n \n         if best_log_prob is None or log_prob_new > best_log_prob:\n             best_parameters = params\n@@ -90,6 +98,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             best_log_prob = log_prob_new\n \n         self.means_ = best_parameters['means']\n+        self.covariances_ = best_parameters['covariances']\n         self.weights_ = best_parameters['weights']\n         self.precisions_cholesky_ = best_parameters['precisions_cholesky_']\n         self.converged_ = True\n@@ -100,6 +109,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return self\n \n     def predict(self, X):\n+        \"\"\"Predict the labels for the data samples in X using trained model.\"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=1)\n         _, log_prob_X = self.score_samples(X)\n@@ -110,6 +120,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return labels\n \n     def predict_proba(self, X):\n+        \"\"\"Return posterior probabilities of membership for each sample.\"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=1)\n         _, log_prob_X = self.score_samples(X)\n@@ -120,6 +131,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return responsibilities\n \n     def score_samples(self, X):\n+        \"\"\"Compute the log probability under the model.\"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=1)\n         n_samples, _ = X.shape\n@@ -130,6 +142,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return log_prob_X\n \n     def _initialize(self, X, init_params, random_state=None):\n+        \"\"\"Initialize model parameters.\"\"\"\n         n_samples, n_features = X.shape\n         n_components = self.n_components\n         weights_init = np.full(n_components, 1. / n_components)\n@@ -140,6 +153,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         precisions_chol_init = np.array([np.eye(n_features) for _ in range(n_components)])\n \n         if 'means' in init_params:\n+            means_init = init_params['means']\n             if isinstance(means_init, str) and means_init == 'kmeans':\n                 kmeans = KMeans(n_clusters=n_components, init='k-means++', n_init=1,\n                                 random_state=random_state)\n@@ -150,6 +164,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 means_init = kmeans.cluster_centers_\n             elif isinstance(means_init, np.ndarray):\n                 means_init = means_init[:n_components]\n+                if means_init.shape != (n_components, n_features):\n+                    raise ValueError('Initial means must have shape (n_components, n_features).')\n             else:\n                 raise ValueError('Invalid initial means.')\n         else:\n@@ -160,6 +175,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         if 'covariances' in init_params:\n+            covariances_init = init_params['covariances']\n             if isinstance(covariances_init, str) and covariances_init == 'diag':\n                 covariances_init = np.array([np.eye(n_features) for _ in range(n_components)])\n             elif isinstance(covariances_init, np.ndarray):\n@@ -170,6 +186,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 covariances_init = covariances_init[:n_components]\n+                if covariances_init.shape != (n_components, n_features, n_features):\n+                    raise ValueError('Initial covariances must have shape (n_components, n_features, n_features).')\n             else:\n                 raise ValueError('Invalid initial covariances.')\n         else:\n@@ -180,6 +197,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return {'weights': weights_init, 'means': means_init,\n                 'covariances': covariances_init, 'precisions_cholesky_': precisions_chol_init}\n \n+    def _check_n_components(self, n_samples):\n+        \"\"\"Check if n_components is valid.\"\"\"\n+        if self.n_components <= 0:\n+            raise ValueError('Number of components must be greater than zero.')\n+        if self.n_components >= n_samples:\n+            raise ValueError('Number of components cannot be greater than or equal to number of samples.')\n+\n     def _e_step(self, X, params):\n+        \"\"\"E-step of the EM algorithm.\"\"\"\n         weights = params['weights']\n         means = params['means']\n         covariances = params['covariances']\n@@ -190,6 +210,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         responsibilities = responsibilities / responsibilities.sum(axis=1, keepdims=True)\n         return responsibilities, log_prob_X\n \n+    def _m_step(self, X, responsibilities, means, covariances, weights):\n+        \"\"\"M-step of the EM algorithm.\"\"\"\n         n_samples, n_features = X.shape\n         n_components = self.n_components\n         reg_covar = self.reg_covar\n@@ -200,6 +221,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         means = np.dot(responsibilities.T, X) / responsibilities.sum(axis=0, keepdims=True)\n         covariances = np.einsum('ij,i,jk->ikj', responsibilities, X - means, X - means) / responsibilities.sum(axis=0, keepdims=True)\n         covariances += reg_covar * np.eye(n_features)[None, :, :]\n+        weights = responsibilities.sum(axis=0) / n_samples\n         precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covariances))",
  "stdout": "",
  "stderr": "",
  "repo_commit": null,
  "docker_image": null,
  "model_config": null,
  "full_result": {
    "task_id": "scikit-learn__scikit-learn-11281",
    "trial_id": 0,
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "prompt_hash": "2e87a44e17ad9283d5acc4eb2fcd2370e6363b188d38c8c7ac691db8586efa35",
    "diff": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 1a2b3c4..5d6e7f8 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -10,6 +10,7 @@\n from sklearn.base import BaseEstimator, ClusterMixin, BiclusterMixin, TransformerMixin\n from sklearn.utils.validation import check_array, check_is_fitted\n from sklearn.utils.extmath import logsumexp\n+from sklearn.cluster._kmeans import _check_cluster_parameters\n \n \n class BaseMixture(BaseEstimator, ClusterMixin):\n@@ -20,6 +21,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n     def __init__(self, n_components=1, tol=1e-3, reg_covar=1e-6,\n                  max_iter=100, n_init=1, init_params='kmeans',\n                  random_state=None, warm_start=False, verbose=0,\n+                 verbose_interval=10, copy_x=True):\n         self.n_components = n_components\n         self.tol = tol\n         self.reg_covar = reg_covar\n@@ -30,6 +32,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.random_state = random_state\n         self.warm_start = warm_start\n         self.verbose = verbose\n+        self.verbose_interval = verbose_interval\n         self.copy_x = copy_x\n \n     def fit(self, X, y=None):\n@@ -40,6 +43,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         \"\"\"\n         Fit the model for X.\n \n+        Parameters\n         X : array-like, shape (n_samples, n_features)\n             Training data.\n \n@@ -50,6 +54,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         \"\"\"\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=2)\n         self._validate_params()\n+        self._check_n_components(X.shape[0])\n \n         # If init_params == 'kmeans', use k-means initialization unless it was\n         # overridden with init_params via a dict setting.\n@@ -60,6 +65,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             init_params = {'means': init_params}\n \n         best_parameters, best_log_prob = None, None\n+        n_trials = 0\n \n         for trial in range(self.n_init):\n             if self.verbose > 0:\n@@ -70,6 +76,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 print(\"Initialization %d\" % (trial + 1))\n \n             params = self._initialize(X, init_params, random_state=self.random_state)\n+            n_trials += 1\n \n             log_prob, responsibilities, means, covariances, weights = self._e_step(X, params)\n             log_prob_new, params_new = self._m_step(X, responsibilities, means, covariances, weights)\n@@ -80,6 +87,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 break\n \n             params = params_new\n+            n_trials += 1\n \n         if best_log_prob is None or log_prob_new > best_log_prob:\n             best_parameters = params\n@@ -90,6 +98,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n             best_log_prob = log_prob_new\n \n         self.means_ = best_parameters['means']\n+        self.covariances_ = best_parameters['covariances']\n         self.weights_ = best_parameters['weights']\n         self.precisions_cholesky_ = best_parameters['precisions_cholesky_']\n         self.converged_ = True\n@@ -100,6 +109,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return self\n \n     def predict(self, X):\n+        \"\"\"Predict the labels for the data samples in X using trained model.\"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=1)\n         _, log_prob_X = self.score_samples(X)\n@@ -110,6 +120,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return labels\n \n     def predict_proba(self, X):\n+        \"\"\"Return posterior probabilities of membership for each sample.\"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=1)\n         _, log_prob_X = self.score_samples(X)\n@@ -120,6 +131,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return responsibilities\n \n     def score_samples(self, X):\n+        \"\"\"Compute the log probability under the model.\"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=1)\n         n_samples, _ = X.shape\n@@ -130,6 +142,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return log_prob_X\n \n     def _initialize(self, X, init_params, random_state=None):\n+        \"\"\"Initialize model parameters.\"\"\"\n         n_samples, n_features = X.shape\n         n_components = self.n_components\n         weights_init = np.full(n_components, 1. / n_components)\n@@ -140,6 +153,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         precisions_chol_init = np.array([np.eye(n_features) for _ in range(n_components)])\n \n         if 'means' in init_params:\n+            means_init = init_params['means']\n             if isinstance(means_init, str) and means_init == 'kmeans':\n                 kmeans = KMeans(n_clusters=n_components, init='k-means++', n_init=1,\n                                 random_state=random_state)\n@@ -150,6 +164,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 means_init = kmeans.cluster_centers_\n             elif isinstance(means_init, np.ndarray):\n                 means_init = means_init[:n_components]\n+                if means_init.shape != (n_components, n_features):\n+                    raise ValueError('Initial means must have shape (n_components, n_features).')\n             else:\n                 raise ValueError('Invalid initial means.')\n         else:\n@@ -160,6 +175,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         if 'covariances' in init_params:\n+            covariances_init = init_params['covariances']\n             if isinstance(covariances_init, str) and covariances_init == 'diag':\n                 covariances_init = np.array([np.eye(n_features) for _ in range(n_components)])\n             elif isinstance(covariances_init, np.ndarray):\n@@ -170,6 +186,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n                 covariances_init = covariances_init[:n_components]\n+                if covariances_init.shape != (n_components, n_features, n_features):\n+                    raise ValueError('Initial covariances must have shape (n_components, n_features, n_features).')\n             else:\n                 raise ValueError('Invalid initial covariances.')\n         else:\n@@ -180,6 +197,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         return {'weights': weights_init, 'means': means_init,\n                 'covariances': covariances_init, 'precisions_cholesky_': precisions_chol_init}\n \n+    def _check_n_components(self, n_samples):\n+        \"\"\"Check if n_components is valid.\"\"\"\n+        if self.n_components <= 0:\n+            raise ValueError('Number of components must be greater than zero.')\n+        if self.n_components >= n_samples:\n+            raise ValueError('Number of components cannot be greater than or equal to number of samples.')\n+\n     def _e_step(self, X, params):\n+        \"\"\"E-step of the EM algorithm.\"\"\"\n         weights = params['weights']\n         means = params['means']\n         covariances = params['covariances']\n@@ -190,6 +210,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         responsibilities = responsibilities / responsibilities.sum(axis=1, keepdims=True)\n         return responsibilities, log_prob_X\n \n+    def _m_step(self, X, responsibilities, means, covariances, weights):\n+        \"\"\"M-step of the EM algorithm.\"\"\"\n         n_samples, n_features = X.shape\n         n_components = self.n_components\n         reg_covar = self.reg_covar\n@@ -200,6 +221,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         means = np.dot(responsibilities.T, X) / responsibilities.sum(axis=0, keepdims=True)\n         covariances = np.einsum('ij,i,jk->ikj', responsibilities, X - means, X - means) / responsibilities.sum(axis=0, keepdims=True)\n         covariances += reg_covar * np.eye(n_features)[None, :, :]\n+        weights = responsibilities.sum(axis=0) / n_samples\n         precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covariances))",
    "patch_lines_added": 0,
    "patch_lines_removed": 0,
    "files_changed": 0,
    "timestamp": "2026-02-17T13:35:34.285951",
    "seed": 42,
    "repo": "scikit-learn/scikit-learn",
    "base_commit": "4143356c3c51831300789e4fdf795d83716dbab6",
    "taxonomy_version": "B-v2-step2-2",
    "gen_elapsed_sec": 20.53550934791565,
    "context_used": true,
    "context_num_files": 80,
    "repo_context_preview": "Existing files (choose from these):\nsetup.py\nconftest.py\ndoc/conf.py\ndoc/conftest.py\nsklearn/base.py\nsklearn/dummy.py\nsklearn/setup.py\nsklearn/_config.py\nsklearn/__init__.py\nsklearn/conftest.py\nsklearn/isotonic.py\nsklearn/pipeline.py\nsklearn/svm/base.py\nsklearn/svm/setup.py\nsklearn/tree/tree.py\nsklearn/exceptions.py\nsklearn/multiclass.py\nsklearn/svm/bounds.py\nsklearn/tree/setup.py\nsklearn/calibration.py\n... (+60 more)",
    "format_used": true,
    "format_ok": false,
    "format_reason": "formatter_exception:Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 4096 tokens and your request has 2488 input tokens (2048 > 4096 - 2488). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
    "apply_check_ok": false,
    "apply_check_reason": "",
    "stdout": "",
    "stderr": "",
    "returncode": null,
    "timeout": false,
    "elapsed_sec": 20.53550934791565,
    "signature": "formatter_exception",
    "test_command": "",
    "stage": "GEN",
    "error_type": "GEN_FAIL",
    "success": false
  }
}