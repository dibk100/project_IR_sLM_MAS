{
  "task_id": "scikit-learn__scikit-learn-13241",
  "trial_id": 0,
  "issue_text": null,
  "test_command": "",
  "diff": "",
  "stdout": "",
  "stderr": "LLM call failed (provider=vllm, base_url=http://localhost:8000/v1, model=Qwen/Qwen2.5-Coder-7B-Instruct): Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 4096 tokens and your request has 3918 input tokens (2048 > 4096 - 3918). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
  "repo_commit": null,
  "docker_image": null,
  "model_config": null,
  "full_result": {
    "task_id": "scikit-learn__scikit-learn-13241",
    "trial_id": 0,
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "prompt_hash": "2524a2883718afc656e9b96fa6a043faed86eb8f2fe464964ebed1f7999510dc",
    "diff": "",
    "patch_lines_added": 0,
    "patch_lines_removed": 0,
    "files_changed": 0,
    "timestamp": "2026-02-12T16:08:22.231197",
    "seed": 42,
    "repo": "scikit-learn/scikit-learn",
    "base_commit": "f8b108d0c6f2f82b2dc4e32a6793f9d9ac9cf2f4",
    "taxonomy_version": "B-v2",
    "gen_elapsed_sec": 0.010779857635498047,
    "stdout": "",
    "stderr": "LLM call failed (provider=vllm, base_url=http://localhost:8000/v1, model=Qwen/Qwen2.5-Coder-7B-Instruct): Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 4096 tokens and your request has 3918 input tokens (2048 > 4096 - 3918). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
    "returncode": null,
    "timeout": false,
    "elapsed_sec": 0.010779857635498047,
    "signature": "llm_call_fail",
    "test_command": "",
    "stage": "GEN",
    "error_type": "GEN_FAIL",
    "success": false
  }
}