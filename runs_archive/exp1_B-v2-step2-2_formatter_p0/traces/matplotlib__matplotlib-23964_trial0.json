{
  "task_id": "matplotlib__matplotlib-23964",
  "trial_id": 0,
  "context_used": true,
  "context_num_files": 80,
  "repo_context_preview": "Existing files (choose from these):\nsetup.py\ntests.py\ndoc/conf.py\nsetupext.py\nlib/pylab.py\ntools/gh_api.py\ntools/subset.py\ntools/memleak.py\ntools/embed_js.py\ntools/make_icons.py\nlib/matplotlib/cm.py\ntools/boilerplate.py\nlib/matplotlib/_cm.py\nlib/matplotlib/afm.py\ntools/github_stats.py\ntools/run_examples.py\ntools/triage_tests.py\ntutorials/text/pgf.py\nlib/matplotlib/_afm.py\nlib/matplotlib/axis.py\n... (+60 more)",
  "format_used": true,
  "format_ok": false,
  "format_reason": "formatter_exception: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 4096 tokens and your request has 2096 input tokens (2048 > 4096 - 2096). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
  "issue_text": null,
  "test_command": "",
  "diff": "diff --git a/lib/matplotlib/backend_ps.py b/lib/matplotlib/backend_ps.py\nindex 1a2b3c4..5d6e7f8 100644\n--- a/lib/matplotlib/backend_ps.py\n+++ b/lib/matplotlib/backend_ps.py\n@@ -666,6 +666,7 @@ class RendererPS(RendererBase):\n             if curr_stream:\n                 for ps_name, xs_names in stream:\n                     yield ps_name, xs_names\n+                else:\n+                    continue\n         except StopIteration:\n             pass",
  "stdout": "",
  "stderr": "",
  "repo_commit": null,
  "docker_image": null,
  "model_config": null,
  "full_result": {
    "task_id": "matplotlib__matplotlib-23964",
    "trial_id": 0,
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "prompt_hash": "d4de9b0228812b57ddf87d98253843adbf6c35717406983e0f5c10092b75743c",
    "diff": "diff --git a/lib/matplotlib/backend_ps.py b/lib/matplotlib/backend_ps.py\nindex 1a2b3c4..5d6e7f8 100644\n--- a/lib/matplotlib/backend_ps.py\n+++ b/lib/matplotlib/backend_ps.py\n@@ -666,6 +666,7 @@ class RendererPS(RendererBase):\n             if curr_stream:\n                 for ps_name, xs_names in stream:\n                     yield ps_name, xs_names\n+                else:\n+                    continue\n         except StopIteration:\n             pass",
    "patch_lines_added": 0,
    "patch_lines_removed": 0,
    "files_changed": 0,
    "timestamp": "2026-02-17T08:22:17.159942",
    "seed": 42,
    "repo": "matplotlib/matplotlib",
    "base_commit": "269c0b94b4fcf8b1135011c1556eac29dc09de15",
    "taxonomy_version": "B-v2-step2-2",
    "gen_elapsed_sec": 1.3788163661956787,
    "context_used": true,
    "context_num_files": 80,
    "repo_context_preview": "Existing files (choose from these):\nsetup.py\ntests.py\ndoc/conf.py\nsetupext.py\nlib/pylab.py\ntools/gh_api.py\ntools/subset.py\ntools/memleak.py\ntools/embed_js.py\ntools/make_icons.py\nlib/matplotlib/cm.py\ntools/boilerplate.py\nlib/matplotlib/_cm.py\nlib/matplotlib/afm.py\ntools/github_stats.py\ntools/run_examples.py\ntools/triage_tests.py\ntutorials/text/pgf.py\nlib/matplotlib/_afm.py\nlib/matplotlib/axis.py\n... (+60 more)",
    "format_used": true,
    "format_ok": false,
    "format_reason": "formatter_exception: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 2048. This model's maximum context length is 4096 tokens and your request has 2096 input tokens (2048 > 4096 - 2096). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
    "stdout": "",
    "stderr": "",
    "returncode": null,
    "timeout": false,
    "elapsed_sec": 1.3788163661956787,
    "signature": "invalid_diff_format",
    "test_command": "",
    "stage": "GEN",
    "error_type": "GEN_FAIL",
    "success": false
  }
}