{
  "task_id": "scikit-learn__scikit-learn-11281",
  "trial_id": 0,
  "issue_text": null,
  "test_command": "git apply",
  "diff": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 1a2b3c4..5d6e7f8 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -10,6 +10,7 @@\n from sklearn.base import BaseEstimator, ClusterMixin, BiclusterMixin, TransformerMixin\n from sklearn.utils.validation import check_array, check_is_fitted\n from sklearn.utils.extmath import logsumexp\n+from sklearn.cluster._kmeans import _check_sample_weight\n \n \n class BaseMixture(BaseEstimator, ClusterMixin):\n@@ -20,6 +21,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n     def __init__(self, n_components=1, tol=1e-3, reg_covar=1e-6,\n                  max_iter=100, n_init=1, init_params='kmeans',\n                  random_state=None, warm_start=False, verbose=0,\n+                 verbose_interval=10, copy_x=True, n_jobs=None,\n                  algorithm='em'):\n         self.n_components = n_components\n         self.tol = tol\n@@ -39,6 +41,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.random_state = random_state\n         self.warm_start = warm_start\n         self.verbose = verbose\n+        self.verbose_interval = verbose_interval\n         self.copy_x = copy_x\n         self.n_jobs = n_jobs\n         self.algorithm = algorithm\n \n     def fit(self, X, y=None, sample_weight=None):\n         \"\"\"Fit the model to the data using EM algorithm.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         y : Ignored\n             Not used, present for API consistency by convention.\n \n         sample_weight : array-like, shape (n_samples,), optional\n             The weights for each observation in X. If None, all observations\n             are assigned equal weight.\n \n         Returns\n         -------\n         self : object\n             The fitted mixture model instance.\n         \"\"\"\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=2)\n         self._validate_params()\n \n         # Initialize parameters\n         self._initialize_parameters(X, sample_weight)\n \n         for i in range(self.max_iter):\n             prev_parameters = deepcopy(self.weights_, self.covariances_,\n                                       self.means_)\n             self._e_step(X, sample_weight)\n             self._m_step(X, sample_weight)\n             self._check_convergence(prev_parameters, i)\n \n         return self\n \n     def predict(self, X):\n         \"\"\"Predict the labels for the data samples in X using trained model.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         labels : array, shape (n_samples,)\n             Component labels.\n         \"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob_norm, log_resp = self._estimate_log_prob(X)\n         return log_resp.argmax(axis=1)\n \n     def predict_proba(self, X):\n         \"\"\"Return posterior probabilities of classification.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         responsibilities : array, shape (n_samples, n_components)\n             Posterior probabilities of classification per component for each\n             sample.\n         \"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob_norm, log_resp = self._estimate_log_prob(X)\n         return np.exp(log_resp - log_prob_norm)\n \n     def score_samples(self, X):\n         \"\"\"Compute the weighted log probability under the model.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         log_prob : array, shape (n_samples,)\n             Log probabilities of each data point in X under the model.\n         \"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob_norm, _ = self._estimate_log_prob(X)\n         return log_prob_norm + logsumexp(self._estimate_log_weights() +\n                                           self._estimate_log_prob(X), axis=1)\n \n     def score(self, X, y=None):\n         \"\"\"Compute the weighted average of log probabilities for X under the model.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         y : Ignored\n             Not used, present for API consistency by convention.\n \n         Returns\n         -------\n         log_likelihood : float\n             Log likelihood of X under the model.\n         \"\"\"\n         return self.score_samples(X).mean()\n \n     def _e_step(self, X, sample_weight):\n         \"\"\"E step.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         sample_weight : array-like, shape (n_samples,)\n             The weights for each observation in X.\n         \"\"\"\n         raise NotImplementedError\n \n     def _m_step(self, X, sample_weight):\n         \"\"\"M step.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         sample_weight : array-like, shape (n_samples,)\n             The weights for each observation in X.\n         \"\"\"\n         raise NotImplementedError\n \n     def _check_convergence(self, prev_parameters, iteration):\n         \"\"\"Check for convergence of the EM algorithm.\n \n         Parameters\n         ----------\n         prev_parameters : tuple\n             Tuple containing previous values of the parameters.\n \n         iteration : int\n             Current iteration number.\n         \"\"\"\n         raise NotImplementedError\n \n     def _initialize_parameters(self, X, sample_weight):\n         \"\"\"Initialize the model parameters.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         sample_weight : array-like, shape (n_samples,)\n             The weights for each observation in X.\n         \"\"\"\n         raise NotImplementedError\n \n     def _estimate_log_prob(self, X):\n         \"\"\"Estimate the log probability of each sample.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         log_prob : array, shape (n_samples, n_components)\n             Log probabilities of each sample under the model.\n         \"\"\"\n         raise NotImplementedError\n \n     def _estimate_log_weights(self):\n         \"\"\"Estimate the log of the weights.\n \n         Returns\n         -------\n         log_weights : array, shape (n_components,)\n             Log of the weights.\n         \"\"\"\n         raise NotImplementedError\n \n     def _check_parameters(self):\n         \"\"\"Check the validity of the parameters.\"\"\"\n         if self.n_components <= 0:\n             raise ValueError('Invalid value for `n_components`: %d'\n                              ' Estimation requires at least one component.'\n                              % self.n_components)\n         if self.tol < 0:\n             raise ValueError('Invalid value for `tol`: %.5f'\n                              ' Tolerance used by the EM must be non-negative.'\n                              % self.tol)\n         if self.reg_covar < 0:\n             raise ValueError('Invalid value for `reg_covar`: %.5f'\n                              ' Covariance regularization must be non-negative.'\n                              % self.reg_covar)\n         if self.max_iter <= 0:\n             raise ValueError('Invalid value for `max_iter`: %d'\n                              ' Maximum number of iterations must be positive.'\n                              % self.max_iter)\n         if self.n_init <= 0:\n             raise ValueError('Invalid value for `n_init`: %d'\n                              ' Number of initializations must be positive.'\n                              % self.n_init)\n         if self.init_params not in ['kmeans', 'random']:\n             raise ValueError('Invalid value for `init_params`: %r'\n                              ' It must be either \\'kmeans\\' or \\'random\\'.'\n                              % self.init_params)\n         if self.random_state is not None:\n             check_random_state(self.random_state)\n         if self.n_jobs is not None:\n             check_n_jobs(self.n_jobs)\n \n     def _validate_params(self):\n         \"\"\"Validate input parameters.\"\"\"\n         self._check_parameters()\n \n     def _check_fit(self):\n         \"\"\"Check if the model has been fitted.\"\"\"\n         check_is_fitted(self)\n \n     def _get_tags(self):\n         \"\"\"Get tags for this estimator.\n \n         Returns\n         -------\n         tags : dict\n             Tags for this estimator.\n         \"\"\"\n         return {'no_validation': True}\n \n \n class GaussianMixture(BaseMixture):\n     \"\"\"Gaussian Mixture Model.\n \n     A Gaussian Mixture Model represents a collection of Gaussian distributions\n     along with their mixing proportions.\n \n     Read more in the",
  "stdout": "",
  "stderr": "Git Apply Failed: error: patch failed: sklearn/mixture/base.py:10\nerror: sklearn/mixture/base.py: patch does not apply\n",
  "repo_commit": null,
  "docker_image": "swebench/sweb.eval.x86_64:latest",
  "model_config": null,
  "full_result": {
    "task_id": "scikit-learn__scikit-learn-11281",
    "trial_id": 0,
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "prompt_hash": "2e87a44e17ad9283d5acc4eb2fcd2370e6363b188d38c8c7ac691db8586efa35",
    "diff": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex 1a2b3c4..5d6e7f8 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -10,6 +10,7 @@\n from sklearn.base import BaseEstimator, ClusterMixin, BiclusterMixin, TransformerMixin\n from sklearn.utils.validation import check_array, check_is_fitted\n from sklearn.utils.extmath import logsumexp\n+from sklearn.cluster._kmeans import _check_sample_weight\n \n \n class BaseMixture(BaseEstimator, ClusterMixin):\n@@ -20,6 +21,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n     def __init__(self, n_components=1, tol=1e-3, reg_covar=1e-6,\n                  max_iter=100, n_init=1, init_params='kmeans',\n                  random_state=None, warm_start=False, verbose=0,\n+                 verbose_interval=10, copy_x=True, n_jobs=None,\n                  algorithm='em'):\n         self.n_components = n_components\n         self.tol = tol\n@@ -39,6 +41,7 @@ class BaseMixture(BaseEstimator, ClusterMixin):\n         self.random_state = random_state\n         self.warm_start = warm_start\n         self.verbose = verbose\n+        self.verbose_interval = verbose_interval\n         self.copy_x = copy_x\n         self.n_jobs = n_jobs\n         self.algorithm = algorithm\n \n     def fit(self, X, y=None, sample_weight=None):\n         \"\"\"Fit the model to the data using EM algorithm.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         y : Ignored\n             Not used, present for API consistency by convention.\n \n         sample_weight : array-like, shape (n_samples,), optional\n             The weights for each observation in X. If None, all observations\n             are assigned equal weight.\n \n         Returns\n         -------\n         self : object\n             The fitted mixture model instance.\n         \"\"\"\n         X = check_array(X, dtype=[np.float64, np.float32], ensure_min_samples=2)\n         self._validate_params()\n \n         # Initialize parameters\n         self._initialize_parameters(X, sample_weight)\n \n         for i in range(self.max_iter):\n             prev_parameters = deepcopy(self.weights_, self.covariances_,\n                                       self.means_)\n             self._e_step(X, sample_weight)\n             self._m_step(X, sample_weight)\n             self._check_convergence(prev_parameters, i)\n \n         return self\n \n     def predict(self, X):\n         \"\"\"Predict the labels for the data samples in X using trained model.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         labels : array, shape (n_samples,)\n             Component labels.\n         \"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob_norm, log_resp = self._estimate_log_prob(X)\n         return log_resp.argmax(axis=1)\n \n     def predict_proba(self, X):\n         \"\"\"Return posterior probabilities of classification.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         responsibilities : array, shape (n_samples, n_components)\n             Posterior probabilities of classification per component for each\n             sample.\n         \"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob_norm, log_resp = self._estimate_log_prob(X)\n         return np.exp(log_resp - log_prob_norm)\n \n     def score_samples(self, X):\n         \"\"\"Compute the weighted log probability under the model.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         log_prob : array, shape (n_samples,)\n             Log probabilities of each data point in X under the model.\n         \"\"\"\n         check_is_fitted(self)\n         X = check_array(X, dtype=[np.float64, np.float32])\n         _, log_prob_norm, _ = self._estimate_log_prob(X)\n         return log_prob_norm + logsumexp(self._estimate_log_weights() +\n                                           self._estimate_log_prob(X), axis=1)\n \n     def score(self, X, y=None):\n         \"\"\"Compute the weighted average of log probabilities for X under the model.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         y : Ignored\n             Not used, present for API consistency by convention.\n \n         Returns\n         -------\n         log_likelihood : float\n             Log likelihood of X under the model.\n         \"\"\"\n         return self.score_samples(X).mean()\n \n     def _e_step(self, X, sample_weight):\n         \"\"\"E step.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         sample_weight : array-like, shape (n_samples,)\n             The weights for each observation in X.\n         \"\"\"\n         raise NotImplementedError\n \n     def _m_step(self, X, sample_weight):\n         \"\"\"M step.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         sample_weight : array-like, shape (n_samples,)\n             The weights for each observation in X.\n         \"\"\"\n         raise NotImplementedError\n \n     def _check_convergence(self, prev_parameters, iteration):\n         \"\"\"Check for convergence of the EM algorithm.\n \n         Parameters\n         ----------\n         prev_parameters : tuple\n             Tuple containing previous values of the parameters.\n \n         iteration : int\n             Current iteration number.\n         \"\"\"\n         raise NotImplementedError\n \n     def _initialize_parameters(self, X, sample_weight):\n         \"\"\"Initialize the model parameters.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         sample_weight : array-like, shape (n_samples,)\n             The weights for each observation in X.\n         \"\"\"\n         raise NotImplementedError\n \n     def _estimate_log_prob(self, X):\n         \"\"\"Estimate the log probability of each sample.\n \n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n             List of n_features-dimensional data points. Each row corresponds\n             to a single data point.\n \n         Returns\n         -------\n         log_prob : array, shape (n_samples, n_components)\n             Log probabilities of each sample under the model.\n         \"\"\"\n         raise NotImplementedError\n \n     def _estimate_log_weights(self):\n         \"\"\"Estimate the log of the weights.\n \n         Returns\n         -------\n         log_weights : array, shape (n_components,)\n             Log of the weights.\n         \"\"\"\n         raise NotImplementedError\n \n     def _check_parameters(self):\n         \"\"\"Check the validity of the parameters.\"\"\"\n         if self.n_components <= 0:\n             raise ValueError('Invalid value for `n_components`: %d'\n                              ' Estimation requires at least one component.'\n                              % self.n_components)\n         if self.tol < 0:\n             raise ValueError('Invalid value for `tol`: %.5f'\n                              ' Tolerance used by the EM must be non-negative.'\n                              % self.tol)\n         if self.reg_covar < 0:\n             raise ValueError('Invalid value for `reg_covar`: %.5f'\n                              ' Covariance regularization must be non-negative.'\n                              % self.reg_covar)\n         if self.max_iter <= 0:\n             raise ValueError('Invalid value for `max_iter`: %d'\n                              ' Maximum number of iterations must be positive.'\n                              % self.max_iter)\n         if self.n_init <= 0:\n             raise ValueError('Invalid value for `n_init`: %d'\n                              ' Number of initializations must be positive.'\n                              % self.n_init)\n         if self.init_params not in ['kmeans', 'random']:\n             raise ValueError('Invalid value for `init_params`: %r'\n                              ' It must be either \\'kmeans\\' or \\'random\\'.'\n                              % self.init_params)\n         if self.random_state is not None:\n             check_random_state(self.random_state)\n         if self.n_jobs is not None:\n             check_n_jobs(self.n_jobs)\n \n     def _validate_params(self):\n         \"\"\"Validate input parameters.\"\"\"\n         self._check_parameters()\n \n     def _check_fit(self):\n         \"\"\"Check if the model has been fitted.\"\"\"\n         check_is_fitted(self)\n \n     def _get_tags(self):\n         \"\"\"Get tags for this estimator.\n \n         Returns\n         -------\n         tags : dict\n             Tags for this estimator.\n         \"\"\"\n         return {'no_validation': True}\n \n \n class GaussianMixture(BaseMixture):\n     \"\"\"Gaussian Mixture Model.\n \n     A Gaussian Mixture Model represents a collection of Gaussian distributions\n     along with their mixing proportions.\n \n     Read more in the",
    "patch_lines_added": 3,
    "patch_lines_removed": 0,
    "files_changed": 1,
    "timestamp": "2026-02-12T22:56:03.274171",
    "seed": 42,
    "repo": "scikit-learn/scikit-learn",
    "base_commit": "4143356c3c51831300789e4fdf795d83716dbab6",
    "taxonomy_version": "B-v2-step2-B",
    "gen_elapsed_sec": 20.4852077960968,
    "stage": "PATCH",
    "stdout": "",
    "stderr": "Git Apply Failed: error: patch failed: sklearn/mixture/base.py:10\nerror: sklearn/mixture/base.py: patch does not apply\n",
    "returncode": 1,
    "timeout": false,
    "elapsed_sec": 0.4685502052307129,
    "test_command": "git apply",
    "success": false,
    "error_type": "PATCH_FAIL",
    "signature": "git_apply_hunk_failed",
    "docker_image": "swebench/sweb.eval.x86_64:latest"
  }
}